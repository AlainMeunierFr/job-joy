---
description: Lead Dev - Outils, stack technique et comportements
alwaysApply: false
---

# Outils et Automatisation

## Délégation vers les agents

**Un mécanisme unique (Task) :**

1. **Outil Task (obligatoire)** : déléguer via `Task({ subagent_type, description, prompt })`. C'est le mode normal (RAM), sans fichier de délégation.
2. **Interdiction** : ne pas utiliser de commande/fichier intermédiaire pour déléguer.
3. **État d'avancement** : gérer l'état des US en mémoire de session (RAM), sans fichier d'état.

**Agents** : `.cursor/agents/` — nom = champ `name` du frontmatter (US, BDD, TDD-back-end, TDD-front-end, Designer).

## Stack Technique (à adapter au projet)

- **IDE** : Cursor
- **Langage** : selon le projet (ex. TypeScript)
- **Frontend / Backend** : selon le projet (ex. Next.js, React, Node)
- **Gestion du code** : Git (ex. GitHub)
- **TDD** : selon le projet (ex. Jest, Vitest)
- **BDD** : selon le projet (ex. Cucumber.js, Playwright)
- **Hébergement** : selon le projet

## Recherche dans le codebase

Avant de modifier :
1. `codebase_search` pour comprendre contexte
2. Lire fichiers pertinents
3. Vérifier tests existants
4. Comprendre architecture

## Création de todos

Pour tâches avec 3+ étapes :
1. Créer liste avec `todo_write`
2. Marquer `in_progress` / `completed`

## Git

Après chaque modification validée :
1. `git add -A`
2. `git commit -m "Message descriptif"`
3. `git push`

**Exception** : Si utilisateur demande de ne pas publier

# Comportements à NE PAS Faire

- ❌ Coder directement sans déléguer
- ❌ Créer fichiers `.md` de documentation sauf demande explicite
- ❌ Modifier fichiers non demandés
- ❌ Lancer tous les tests à chaque modification
- ❌ Refactoriser sans demande (sauf Boy Scout : nettoyer en passant)

# Gestion des erreurs

1. **Avant d’analyser des échecs de pipeline ou de tests** : lire **logs du pipeline (ex. `logs/*-errors.txt`)** s'ils existent. S'en servir pour identifier l’étape en échec et la cause — ne pas relancer la suite de tests pour « retrouver » les erreurs.
2. Pour le détail des tests : utiliser **fichiers de résultats** du projet (ex. `test-results.json`, `coverage/coverage-summary.json`) plutôt que de relancer les tests en premier.
3. Identifier problème métier/fonctionnel AVANT solution technique.
4. Analyser erreur complètement.
5. Pas de corrections multiples en parallèle.
6. Solution simple avant solution complexe.
7. Si échec multiple → repartir de zéro avec User Story → Plan de test → TDD.
